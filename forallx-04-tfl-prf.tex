\chapter{Natural Deduction for TFL}\label{ch:NDTFL}

\section{The Idea Behind Natural Deduction}\label{s:NDVeryIdea}


We've seen how to use truth tables to determine whether a TFL argument is valid.  Truth tables are nice because they give us a completely mechanical test for validity: we just crunch through the table and see whether there is any valuation that makes all the premises true and the conclusion false.   But truth tables don't give us much \emph{insight} into why arguments are valid.

A rather different approach to logic is to try to \emph{deduce} the conclusion from the premises via a series of simple inferences.  Think of this as the Sherlock Holmes approach to logic: given some evidence (premises), Holmes methodically draws one inference after another, until he arrives at a conclusion about who committed the crime.  If all the inferences in the deduction are correct, the conclusion must follow from the premises Holmes started with.

%Consider these two TFL arguments:
%\begin{align*}
%P \eif Q, P & \therefore Q\\
%P \eor Q, \enot P & \therefore Q
%\end{align*}
%Clearly, these are valid. You could confirm that they are by constructing four-line truth tables. But another way to tell that they are valid is by noticing that they make use of intuitively correct \emph{forms of inference}.  The first form of inference goes by the fancy Latin name of \emph{modus ponens}, and the second is called \emph{disjunctive syllogism} (its Latin name is \emph{modus tollendo ponens}).

%This suggests a rather different approach to logic.  Rather than constructing a truth table, we can assess an argument's validity by seeing whether it is possible to \emph{deduce} its conclusion from its premises via a series of simple, intuitively correct  steps of inference.  Think of it as the Sherlock Holmes approach to logic, in contrast to the more computational approach embodied by truth tables.

What we will do in this chapter is to introduce a \define{natural deduction} system that formalizes this process of deducing things from premises. We'll introduce a few very basic rules of inference, which can then be combined into more complicated chains of reasoning. Indeed, with just a small set of rules, we will be able to capture \emph{all} valid arguments.   Whereas truth tables are completely mechanical, natural deduction requires insight and ingenuity.  This makes it harder, but also more interesting and rewarding.

The move to natural deduction can be motivated by more than the search for insight, however. It might also be motivated by necessity. In TFL, truth tables give us a  completely mechanical test for validity.  Of course they can get unmanageably big as the number of atomic sentences increase, but you could in principle program a computer to crunch through them for you.   When we get to FOL, in the second part of this book, things will look very different. There is nothing like the truth table test for validity available in FOL.  The fact that there is no completely mechanical test for validity in FOL is a deep mathematical result, independently proved by Alan Turing and Alonzo Church in 1936.  So in FOL, using methods that require ingenuity and insight become indispensable, and we will have to rely on natural deduction to prove arguments valid.

The modern development of natural deduction dates from simultaneous papers from 1934 by Gerhard Gentzen and Stanis\l{}aw Ja\'{s}kowski.   Later, in 1952, Frederic Fitch introduced the graphical ``Fitch notation'' for natural deduction proofs that we will use here.  %With this background out of the way, let's look at our system of deduction!



\section{Setting up Natural Deduction Proofs}\label{s:BasicTFL}

The \define{natural deduction} system we will develop includes a pair of rules for every connective in the language. \define{Introduction rules}  allow us to prove a sentence that has that connective as the main logical operator, and \define{elimination rules} allow us to prove something \emph{from} a sentence that has that connective as the main logical operator.  %Natural deduction proofs are a bit like chess problems.  Just as different chess pieces come with different rules governing their movement, so the connectives come with different rules governing them.  What we'll have to learn is how to put these rules together so as to reach the conclusion we're aiming at from the premises we're given.

Our natural deduction proofs will be \emph{formal proofs}.  They will consist of a sequence of lines, with the premises  listed at the top and the conclusion at the bottom.  All the lines in between have to be justified as following from earlier lines via some rule of inference. As an illustration, consider the following instance of DeMorgan's Law:
	$$\enot (A \eor B) \therefore \enot A \eand \enot B$$
We would start this proof by writing the premise:
\begin{proof}
	\hypo{a1}{\enot (A \eor B)} \by{Premise}{}
\end{proof}
Note that we have numbered the premise, since we will want to refer back to it. Indeed, every line on a proof is numbered so that we can refer back to it.  Note also that we have drawn a vertical line to the left and a horizontal line underneath the premise. Everything written above the horizontal line is an \emph{assumption} --- so the premise is introduced into the proof as an assumption. Everything written below the horizontal line will either be something which follows from this assumption, or it will be some new assumption.

We are hoping to conclude that `$\enot A \eand \enot B$'.  So we are hoping ultimately to end our proof with a line that looks like this:
\begin{proof}
	\have[n]{con}{\enot A \eand \enot B}
\end{proof}
for some line number $n$. It doesn't matter what line number we end on, but we would obviously prefer a short proof to a long one.

Or to take another example, suppose we wanted to prove that the following is valid:
$$A\eor B, \enot (A\eand C), \enot (B \eand \enot D) \therefore \enot C\eor D$$
The argument has three premises, so we start by writing them all down, numbered, and drawing a vertical line the the left and a horizontal line underneath:
\begin{proof}
	\hypo{a1}{A \eor B}  \by{Premise}{}
	\hypo{a2}{\enot (A\eand C)} \by{Premise}{}
	\hypo{a3}{\enot (B \eand \enot D)} \by{Premise}{}
\end{proof}
We are aiming to conclude with a line that looks like this:
\begin{proof}
	\have[n]{con}{\enot C \eor D}
\end{proof}
What we have to learn are rules of inference, and how to chain them together to move in a step-by-step fashion from the premises to the conclusion.

Before we look at the rules, however, we'll introduce some new terminology and notation having to do with proofs.  We will use the following expression:
$$\meta{\varphi}_1, \ldots, \meta{\varphi}_n \proves \meta{\psi}$$
to mean that $\meta{\psi}$ is \emph{provable} from $\meta{\varphi}_1, \ldots, \meta{\varphi}_n$.  That is, that there \emph{exists a proof} which ends with $\meta{\psi}$ and whose premises include at most $\meta{\varphi}_1, \ldots, \meta{\varphi}_n$. We'll call a provability claim of this form a \define{sequent}.  By providing a natural deduction proof, we can demonstrate that a sequent holds, i.e. demonstrate that $\meta{\psi}$ is indeed provable from $\meta{\varphi}_1, \ldots, \meta{\varphi}_n$.  When we want to say that $\meta{\psi}$ is \emph{not} provable from $\meta{\varphi}_1, \ldots, \meta{\varphi}_n$, we write:
$$\meta{\varphi}_1, \ldots, \meta{\varphi}_n \nproves \meta{\psi}$$
Natural deduction does not give us a way to verify claims like these, about the \emph{non-existence} of a proof. More complicated reasoning would be required to show this kind of thing.


The symbol `$\proves$' is called the \emph{single turnstile}.  This is \emph{not} the same as the double turnstile symbol `$\entails$' that we used to symbolize entailment in chapter \ref{ch:SemanticsOfTFL}. The single turnstile `$\proves$' says something about the \emph{existence} of a certain kind of \emph{proof} (one that begins with certain premises and ends in a certain conclusion).  The double turnstile `$\entails$' says something about the \emph{non-existence} of a certain kind of \emph{valuation} (one that makes the premises true and the conclusion false).  Valuations are completely different kinds of things from proofs, so it's important not to confuse `$\proves$' (the \emph{proof theoretic} notion of \emph{provability}) with `$\entails$' (the \emph{semantic} notion of \emph{entailment}).

That said, the system of natural deduction that we will develop is designed to deliver a proof whenever a semantic entailment holds.  That is, it is designed to ensure:
\factoidbox{\define{Completeness:} $\text{If\ } \meta{\varphi}_1, \ldots, \meta{\varphi}_n \entails \meta{\psi} \text{ then } \meta{\varphi}_1, \ldots, \meta{\varphi}_n \proves \meta{\psi} $}
meaning that if \meta{\psi} is semantically entailed by $\meta{\varphi}_1, \ldots, \meta{\varphi}_n$, then there must also exist a proof of \meta{\psi} from $\meta{\varphi}_1, \ldots, \meta{\varphi}_n$.    Our natural deduction system is also designed to guarantee the other direction:
\factoidbox{\define{Soundness:} $\text{If\ } \meta{\varphi}_1, \ldots, \meta{\varphi}_n \proves \meta{\psi} \text{ then } \meta{\varphi}_1, \ldots, \meta{\varphi}_n \entails \meta{\psi} $}
meaning that whenever \meta{\psi} is provable from $\meta{\varphi}_1, \ldots, \meta{\varphi}_n$, then \meta{\psi} is also semantically entailed by $\meta{\varphi}_1, \ldots, \meta{\varphi}_n$.\footnote{Note that this is a different notion of soundness from the one we discussed in \S \ref{s:GoodBadArg}.}  All good proof systems should be both sound and complete, to ensure that the proof-theoretic notion of provability matches up perfectly with the semantic notion of entailment. In a more advanced logic class, you learn how to provide ``meta-logical'' proofs showing that a proof system is both sound and complete, but here you'll just have to take my word for it that our natural deduction system will have both of these features.

We also have a proof-theoretic analogue of the semantic notion of TF equivalence:
	\factoidbox{
		Two sentences \meta{\varphi} and \meta{\psi} are \define{provably equivalent} iff each is provable from the other; i.e., both $\meta{\varphi}\proves\meta{\psi}$ and $\meta{\psi}\proves\meta{\varphi}$, also written $\meta{\varphi} \pequiv \meta{\psi}$.}
Given that our natural deduction system is both sound and complete, we will again have it that any two TF equivalent sentences are also provably equivalent, and vice versa.  Let's now look at the rules that constitute our system of natural deduction.







%\section{Reiteration}
%The very first rule is so breathtakingly obvious that it is surprising we bother with it at all.
%
%If you already have shown something in the course of a proof, the \emph{reiteration rule} allows you to repeat it on a new line. For example:
%\begin{proof}
%	\have[4]{a1}{A \eand B}
%	\have[$\vdots$]{}{\vdots}
%	\have[10]{a2}{A \eand B} \by{R}{a1}
%\end{proof}
%This indicates that we have written `$A \eand B$' on line 4. Now, at some later line---line 10, for example---we have decided that we want to repeat this. So we write it down again. We also add a citation which justifies what we have written. In this case, we write `R', to indicate that we are using the reiteration rule, and we write `$4$', to indicate that we have applied it to line $4$.
%
%Here is a general expression of the rule:
%\begin{proof}
%	\have[m]{a}{\meta{\varphi}}
%	\have[\ ]{c}{\meta{\varphi}} \by{R}{a}
%\end{proof}
%The point is that, if any sentence $\meta{\varphi}$ occurs on some line, then we can repeat $\meta{\varphi}$ on later lines. Each line of our proof must be justified by some rule, and here we have `R $m$'. This means: Reiteration, applied to line $m$.
%
%Two things need emphasising. First `$\meta{\varphi}$' is not a sentence of TFL. Rather, it a symbol in the metalanguage, which we use when we want to talk about any sentence of TFL (see \S\ref{s:UseMention}). Second, and similarly, `$m$' is not a numeral that will appear on a proof. Rather, it is a symbol in the metalanguage, which we use when we want to talk about any line number of a proof. In an actual proof, the lines are numbered `$1$', `$2$', `$3$', and so forth. But when we define the rule, we use variables to underscore the point that the rule may be applied at any point.

\section{Conjunction Rules}
Suppose I want to show that Alice is both a logician and a tennis player. One obvious way to do this would be as follows: first I show that Alice is a logician; then I show that Alice is a tennis player; then I put these two demonstrations together to obtain the conjunction.

Our natural deduction system will capture this thought via the rule of $\eand$-Introduction, or $\eand I$ for short. Perhaps I am working through a proof, and have obtained `$H$' on line 8 and `$R$' on line 15. Then on any subsequent line I can obtain `$H \eand R$' thus:
\begin{proof}
	\have[8]{a}{H}
	\have[\ ]{}{\vdots}
	\have[15]{b}{R}
	\have[\ ]{}{\vdots}
	\have[\ ]{c}{H \eand R} \ai{a, b}
\end{proof}
Every line of our proof must either be a premise (or an assumption, as we'll see later), or must be justified by some rule like this. We cite `$\eand$I 8, 15' here to indicate that `$H \eand R$' is obtained by the rule of conjunction introduction applied to lines 8 and 15. We could equally well have conjoined the conjuncts in the opposite order to infer `$R \eand H$' rather than `$H \eand R$', though then we should also adjust our rule citation to read `$\eand$I 15, 8', with the line numbers of the two conjuncts listed in the opposite order.

More generally, our conjunction introduction rule is:

\factoidbox{
\begin{proof}
	\have[m]{a}{\meta{\varphi}}
	\have[n]{b}{\meta{\psi}}
	\have[\ ]{c}{\meta{\varphi} \eand \meta{\psi}} \ai{a,b}
\end{proof}}
Here lines $m$ and $n$ can occur in either order, i.e. \meta{\varphi} could occur first in the proof followed by \meta{\psi} later on, or \meta{\psi} could occur first followed by \meta{\psi} later on.

%To be clear, the statement of the rule is \emph{schematic}. It is not itself a proof.  `$\meta{\varphi}$' and `$\meta{\psi}$' are not sentences of TFL. Rather, they are symbols in the metalanguage, which we use when we want to talk about any sentence of TFL (see \S\ref{s:UseMention}). Similarly, `$m$' and `$n$' are not a numerals that will appear on any actual proof. Rather, they are symbols in the metalanguage, which we use when we want to talk about any line number of any proof. In an actual proof, the lines are numbered `$1$', `$2$', `$3$', and so forth. But when we define the rule, we use variables to emphasise that the rule may be applied at any point. The rule requires only that we have both conjuncts available to us somewhere in the proof. They can be separated from one another, and they can appear in any order.

The rule is called ``conjunction \emph{introduction}'' because it introduces the symbol `$\eand$' into our proof where it may have been absent. Correspondingly, we have a rule that \emph{eliminates} that symbol.  Suppose you have shown that Alice is both a logician and a tennis player. Then you're entitled to infer that Alice is a logician, and you're also entitled to infer that Alice is a tennis player. This gives us our conjunction elimination rule(s):
\factoidbox{
\begin{proof}
	\have[m]{ab}{\meta{\varphi}\eand\meta{\psi}}
	\have[\ ]{a}{\meta{\varphi}} \ae{ab}
\end{proof}}
and equally:
\factoidbox{
\begin{proof}
	\have[m]{ab}{\meta{\varphi}\eand\meta{\psi}}
	\have[\ ]{b}{\meta{\psi}} \ae{ab}
\end{proof}}
The point is simply that, when you have a conjunction on some line in a proof, you can obtain either of its two conjuncts by applying the rule of {\eand}E.

One point is worth emphasizing: you can only apply this rule (as well as the other rules we'll introduce) to the \emph{main logical operator} of a sentence.  So the following would not be a legitimate use of $\eand E$:

\begin{proof}
	\hypo{a1}{P \eand (Q \eand R)}
	\have{b}{R} \ae{a1}
\end{proof}
I can only apply $\eand E$ to the main operator of `$P \eand (Q \eand R)$', giving me either `$P$' or `$(Q \eand R)$'.  I could then apply $\eand E$ to the latter to get `$R$' itself; but I can't get $R$ \emph{directly} from `$P \eand (Q \eand R)$'.

Here's an example that illustrates this. The following argument is valid (showing that $\eand$ is associative):
	$$A \eand (B \eand C) \therefore (A \eand B) \eand C$$
To provide a proof for this argument, we start by writing the premise:
\begin{proof}
	\hypo{ab}{A \eand (B \eand C)}  \by{Premise}{}
\end{proof}
From the premise, we can get both `$A$' and `$(B \eand C)$' by applying $\eand$E twice. And we can then apply $\eand$E twice more to $(B \eand C)$ to get a proof that looks like this:
\begin{proof}
	\hypo{ab}{A \eand (B \eand C)}  \by{Premise}{}
	\have{a}{A} \ae{ab}
	\have{bc}{B \eand C} \ae{ab}
	\have{b}{B} \ae{bc}
	\have{c}{C} \ae{bc}
\end{proof}
Again: we \emph{cannot} get `$B$' or `$C$' by applying $\eand E$ directly to line 1.  We first have to get `$(B \eand C)$' from 1, and then get `$B$' and `$C$' out of \emph{this} by applying $\eand E$ again. To get to our desired conclusion, we now just put the various atomic sentences back together using $\eand I$:
\begin{proof}
	\hypo{abc}{A \eand (B \eand C)}  \by{Premise}{}
	\have{a}{A} \ae{abc}
	\have{bc}{B \eand C} \ae{abc}
	\have{b}{B} \ae{bc}
	\have{c}{C} \ae{bc}
	\have{ab}{A \eand B}\ai{a, b}
	\have{con}{(A \eand B) \eand C}\ai{ab, c}
\end{proof}

Notice that whereas $\eand E$ gets applied to a single line, $\eand I$ gets applied to two lines.  However, $\eand I$ doesn't necessarily have to be applied to two \emph{different} lines.   If we wanted, for example, we could formally prove `$A \eand A$' from `$A$' as follows:
\begin{proof}
	\hypo{a}{A}
	\have{aa}{A \eand A}\ai{a, a}
\end{proof}
And we could now apply $\eand E$ to line 2 to prove the rather uninteresting fact that `$A$' follows from `$A$':

\begin{proof}
	\hypo{a}{A}
	\have{aa}{A \eand A}\ai{a, a}
	\have{a2}{A}\ae{aa}
\end{proof}

\section{Conditional Rules}
Consider the following argument:
	\begin{quote}
		If Jane is smart then she is fast. Jane is smart. So Jane is fast.
	\end{quote}
This argument is certainly valid. And it suggests a conditional elimination rule ($\eif$E):
\factoidbox{
\begin{proof}
	\have[m]{ab}{\meta{\varphi}\eif\meta{\psi}}
	\have[n]{a}{\meta{\varphi}}
	\have[\ ]{b}{\meta{\psi}} \ce{ab,a}
\end{proof}}
This rule implements the \emph{modus ponens} form of inference mentioned earlier: given a conditional, and given its antecedent, we can infer its consequent. Again, this is an elimination rule, because it allows us to obtain a sentence that may not contain `$\eif$', having started with a sentence that did contain `$\eif$'. Note that the conditional and its antecedent can appear in either order in our proof. However, in the citation for $\eif$E, we should  cite the conditional first, followed by the antecedent.

The rule for conditional introduction is also quite easy to motivate. The following argument should be valid:
	\begin{quote}
		Alice is a chemist. Therefore, if Alice is a biologist, then Alice is both a chemist and a biologist.
	\end{quote}
If someone doubted that this was valid, we could try to convince them otherwise by explaining ourselves as follows:
\begin{quote}
We are given that Alice is a chemist. Now, assume additionally, for the sake of argument, that Alice is also a biologist. Then by conjunction introduction, Alice is both a chemist and a biologist. Of course, this only follows given our assumption that Alice is a biologist. So what we've shown is that \emph{if} Alice is a biologist, then she is both a chemist and a biologist.
\end{quote}
Transferred into natural deduction format, here is the pattern of reasoning that we just used. We started with one premise, `Alice is a chemist':
	\begin{proof}
		\hypo{r}{C}  \by{Premise}{}
	\end{proof}
The next thing we did is to make an \emph{temporary assumption} (`Alice is a biologist'), for the sake of argument. To indicate that we are no longer dealing merely with our original premise `$C$', but with an additional assumption, we continue our proof as follows:
	\begin{proof}
		\hypo{r}{C}  \by{Premise}{}
		\open
			\hypo{l}{B} \by{Assumption}{}
			\have[\ ]{}{}
	\end{proof}
Introducing `$B$' as a temporary assumption opens up a \emph{subproof}.  Inside this subproof we can now reason under the assumption, or hypothesis, that `$B$' holds. We indicate this by drawing a line under `$B$' (to indicate that it is an assumption) and by indenting it with a further vertical line (to indicate that we have entered a new subproof headed by this assumption).

With this extra assumption in place, we are in a position to use $\eand$I:
	\begin{proof}
		\hypo{r}{C}  \by{Premise}{}
		\open
			\hypo{l}{B} \by{Assumption}{}
			\have{rl}{C \eand B}\ai{r, l}
%			\close
%		\have{con}{L \eif (R \eand L)}\ci{l-rl}
	\end{proof}
So we have now shown that, under the assumption that `$B$' holds, we can infer `$C \eand B$'. We can therefore conclude that, \emph{if} `$B$' obtains, so does `$C \eand B$'. Or, put another way, we can conclude `$B \eif (C \eand B)$':
	\begin{proof}
		\hypo{r}{C}
		\open
			\hypo{l}{B}
			\have{rl}{C \eand B}\ai{r, l}
			\close
		\have{con}{B \eif (C \eand B)}\ci{l-rl}
	\end{proof}
Notice that we have popped back out of the subproof opened by our assumption.  This indicates that we have now \emph{discharged} the temporary assumption `$B$', and concluded that the conditional itself follows just from our original premise `$C$'.  Although you can in principle always make an assumption, it is absolutely essential that any assumptions you do make be discharged by the end of the proof.  This is why they are \emph{temporary} assumptions.

The general pattern at work here is the following: to prove a conditional $\meta{\varphi} \eif \meta{\psi}$, we \emph{assume} the antecedent \meta{\varphi} temporarily or ``for the sake of argument'' (thereby opening a new subproof), and then try to prove the consequent $\meta{\psi}$ from that assumption. If we succeed, we can discharge the assumption (popping out of the subproof) and conclude that the conditional $\meta{\varphi} \eif \meta{\psi}$ holds:
\factoidbox{
	\begin{proof}
		\open
			\hypo[m]{a}{\meta{\varphi}}
			\have[\ ]{}{\vdots}
			\have[n]{b}{\meta{\psi}}
		\close
		\have[\ ]{ab}{\meta{\varphi}\eif\meta{\psi}}\ci{a-b}
	\end{proof}}

Here's another illustration of $\eif$I in action. Suppose we want to prove:
	$$P \eif Q, Q \eif R \vdash P \eif R$$
We start by listing both of our premises. Then, since we are aiming to prove a conditional, namely, `$P \eif R$', we assume the antecedent to that conditional as an additional assumption, which opens a new subproof:
\begin{proof}
	\hypo{pq}{P \eif Q}  \by{Premise}{}
	\hypo{qr}{Q \eif R}  \by{Premise}{}
	\open
		\hypo{p}{P}  \by{Assumption}{}
		\have[\ ]{}{\vdots}
		\have[\ ]{}{R}
	\close
\end{proof}
Our goal now is to prove $R$ from this temporary assumption, inside of our subproof.  Given `$P$' , we can use {\eif}E on the first premise. This will yield `$Q$'. And we can then use {\eif}E on the second premise to get `$R$'. So, by assuming `$P$' we were able to prove `$R$'!  We can now apply the {\eif}I rule, thereby discharging `$P$' and popping out of the subproof::\label{HSproof}
\begin{proof}
	\hypo{pq}{P \eif Q}  \by{Premise}{}
	\hypo{qr}{Q \eif R}  \by{Premise}{}
	\open
		\hypo{p}{P} \by{Assumption}{}
		\have{q}{Q}\ce{pq,p}
		\have{r}{R}\ce{qr,q}
	\close
	\have{pr}{P \eif R}\ci{p-r}
\end{proof}
Notice that when applying $\eif$I to obtain `$P \eif R$', we have to cite the \emph{entire} subproof that begins with `$P$' and ends with `$R$'.  So we use a dash, rather than just a comma, between the two line numbers (writing `$3$--$5$' rather than `$3,5$').

\section{Additional assumptions and subproofs}
The rule $\eif$I invoked the idea of opening subproofs via additional assumptions. This needs to be handled with some care. Consider this proof:
\begin{proof}
	\hypo{a}{A}  \by{Premise}{}
	\open
		\hypo{b1}{B} \by{Assumption}{}
		\have{bb}{B \eand B}\ai{b1, b1}
		\have{b2}{B} \ae{bb}
	\close
	\have{con}{B \eif B}\ci{b1-b2}
\end{proof}
This is a perfectly legitimate, if somewhat unusual, proof.  What it shows is that the argument $A \therefore B \eif B$ is valid.  This is as it should be: `$B \eif B$' is a tautology, and any argument with a tautology as its conclusion is valid.  But suppose we now tried to continue the proof as follows:
\begin{proof}
	\hypo{a}{A} \by{Premise}{}
	\open
		\hypo{b1}{B} \by{Assumption}{}
		\have{bb}{B \eand B}\ai{b1, b1}
		\have{b2}{B} \ae{bb}
	\close
	\have{con}{B \eif B}\ci{b1-b2}
	\have{b}{B}\by{\textbf{No!!} \ $\eif$E}{con, b2}
\end{proof}
If we were allowed to do this, it would be a disaster: our proof would now purport to show that the argument $A \therefore B$ is valid.   We could in this way prove any conclusion we liked from any premise whatsoever.  That would obviously destroy the soundness of our natural deduction system.

What has gone wrong here is that on line 6 we've illegitimately tried to apply $\eif$E to line 4, which occurs inside a subproof we've already closed off.  A subproof can be thought of as showing what \emph{would} follow if the assumption that opens it held.  While we are working within the subproof, we can refer to the assumption that we made to open the subproof, and to anything that we obtained from our premises. After all, those premises still hold. But once we close the subproof and return to the main proof, the assumption that opened it has been \define{discharged}, and it becomes illegitimate to draw upon anything that depends upon that assumption, i.e. on anything inside the subproof opened by that assumption. Thus we stipulate:
	\factoidbox{To cite any individual line when applying a rule, that line must (1) occur before the application of the rule, but (2) not occur within a closed subproof. }
The application of $\eif$E in the faulty proof above involves citing a line (namely line 4) that occurs within a subproof that has (by line 6) been closed. This is illegitimate.

%In the course of a proof, we therefore have to keep very careful track of what assumptions we are making, and which assumptions have been discharged. Our proof system does this very graphically, via its vertical ``Fitch bars.''  Indeed, that's precisely why we have chosen to use \emph{this} proof system.

Once we have started thinking about what we can show by making additional assumptions, nothing stops us from posing the question of what we could show if we were to make \emph{even more} assumptions. We can in other words introduce a subproof within a subproof. Here is an example of such nested subproofs:
\begin{proof}
\hypo{a}{A}  \by{Premise}{}
\open
	\hypo{b}{B}  \by{Assumption}{}
	\open
		\hypo{c}{C}  \by{Assumption}{}
		\have{ab}{A \eand B}\ai{a,b}
	\close
	\have{cab}{C \eif (A \eand B)}\ci{c-ab}
\close
\have{bcab}{B \eif (C \eif (A \eand B))}\ci{b-cab}
\end{proof}
This proof gets set up as follows: we begin with the premise `$A$' and the goal of proving `$B \eif (C \eif (A \eand B))$'.  Since the conclusion is a conditional, we assume its antecedent `$B$' and set ourselves the new goal of proving its consequent `$(C \eif (A \eand B))$' using this additional assumption.  But our new goal `$(C \eif (A \eand B))$' is \emph{itself} a conditional, so we repeat the same process: assume its antecedent `$C$', and try to prove its consequent `$A \eand B$' in the subproof we've opened.  Proving `$A \eand B$' is easy: we can just apply $\eand $I to our original premise from line 1 and our first assumption from line 2.  Referring back to lines 1 and 2 in step 4 of the proof in this manner is legitimate,  since neither line occurs in a subproof that has been closed by the time of step 4.

But it would now \emph{not} be legitimate to continue the proof as follows:
\begin{proof}
\hypo{a}{A}
\open
	\hypo{b}{B}
	\open
		\hypo{c}{C}
		\have{ab}{A \eand B}\ai{a,b}
	\close
	\have{cab}{C \eif (A \eand B)}\ci{c-ab}
\close
\have{bcab}{B \eif(C \eif (A \eand B))}\ci{b-cab}
\have{bcab}{C \eif (A \eand B)}\by{\textbf{No!!} $\eif$I}{c-ab}
\end{proof}
This would be awful. This proof would purport to show that `$C \eif (A \eand B)$' can be deduced from the premise `$A$'.  But the argument  `$A \therefore C \eif (A \eand B)$' is certainly not valid. Again, if we were allowed to do this kind of thing, our proof system would no longer be sound.

The problem is that the subproof that began with the assumption `$C$' occurs within the scope of (i.e. within the subproof opened by) assumption `$B$' on line 2. By line 6, we have \emph{discharged} assumption `$B$'. So it is cheating to try to help ourselves (on line 7) to a subproof that occurs within the scope of an assumption that has already been discharged.  Here the problem isn't that we cited an individual \emph{line} that occurs inside a closed subproof, but that we cited an entire \emph{subproof} that occurs inside a closed subproof.  So we expand our stipulation to cover rules that cite entire subproofs:
	\factoidbox{To cite a subproof when applying a rule, the subproof must (1) come before the application of the rule, but (2) not occur within some other {closed} subproof.}
Our proof above violates this stipulation, since the subproof of lines 3--4 occurs within the subproof spanning lines 2--5, which has already been closed by the point we get to line 7.


And to again emphasize a point from earlier: although you can, in principle, always make any temporary assumption you like, you always have to ultimately have a way of discharging that assumption and popping out of the subproof that it opens (otherwise it wouldn't be temporary any more!).    For this reason it is very important to \emph{only}  make assumptions when you have a discharge strategyin mind.  At this point, we have only one rule that allows us to make an assumption, and that is the rule of $\eif$I: if your goal is to prove a conditional $\meta{\varphi} \eif \meta{\psi}$, you should assume its antecedent \meta{\varphi} and try to prove its consequent \meta{\psi} inside the subproof opened by that assumption.  But at this point, this is the only time at which you should be making assumptions --- when aiming to prove conditionals.

\practiceproblems
\problempart
Prove the following sequents (these require only conjunction rules and $\eif$E):

\begin{earg}
\item $A \eand (B \eand C) \proves C \eand B$
\item $P \eand Q, (Q \eand P) \eif R \proves R$
\item $A \eand B, B \eif (A \eif C) \proves C \eand B$
 \item $(A\eif ((C\eand  A)\eif D)), (A\eand  C) \vdash D$
\end{earg}

\problempart
Prove the following sequents (these now also require $\eif$I, and note that the last one asks you to prove an equivalence, meaning you have to give proofs in both directions):

\begin{earg}
\item $(A\eif (B\eand  D)), (A\eif C) \vdash (A\eif (C\eand  D))$
\item $A \eif B, B \eif C \proves A \eif C$
\item $A \eif B, B \eif C \proves A \eif (B \eand C)$
\item $(A\eand  B)\eif C, A\eif B \vdash A\eif C$
\item $A \eif B \proves (A \eand C) \eif (B \eand C)$
\item $A \eif (B \eif C) \pequiv (A \eand B) \eif C$
\end{earg}




\section{Proving Theorems and Reiterating}\label{s:TheoremsReiterating}

We said that the sequent $\meta{\varphi}_1, \ldots, \meta{\varphi}_n \proves \meta{\psi}$ means that there exists a proof ending in \meta{\psi} whose premises include at most $\meta{\varphi}_1, \ldots, \meta{\varphi}_n$.  Similarly, we will write:
$${} \proves \meta{\varphi}$$
to say that there is a proof of $\meta{\varphi}$ with no premises whatsoever.  Sentences which are provable with no premises are called \define{Theorems}.  Notice the similarity with our notation in semantics, where we used $\entails \meta{\varphi}$ to say that \meta{\varphi} is a tautology.  This is no accident: given that our system of natural deduction is both is sound and complete, every \emph{tautology} should be a \emph{theorem} of our proof system, and vice versa.

For example, since $(A \eand B) \eif A$ is a tautology, we should be able to prove it with no premises.  Here's how that looks like:\\
\begin{proof}
\open
\hypo{1}{A \eand B}  \by{Assumption}{}
\have{2}{A} \ae{1}
\close
\have{3}{(A \eand B) \eif A)} \ci{1-2}
\end{proof}
\noindent Notice that, unlike in any of the other proofs we've looked at, the leftmost vertical line, which appears next to our conclusion, has no premise listed at its top.  This graphically indicates that `$(A\eand B)\eif A$' is a theorem in our proof system, i.e. something that is provable without any premises.

As another example, take the tautology `$A \eif (B \eif A)$'.  This is provable as a theorem as follows:\\
\begin{proof}
\open
\hypo{1}{A}  \by{Assumption}{}
\open
\hypo{2}{B}  \by{Assumption}{}
\have{3}{A \eand B} \ai{1,2}
\have{4}{A} \ae{3}
\close
\have{5}{B \eif A} \ci{2-4}
\close
\have{6}{A \eif (B \eif A)} \ci{1-5}
\end{proof}
\noindent This proof is a bit odd: since we're trying to prove `$(B \eif A)$' on line 5, the subproof that begins with `$B$' on line 2 has to end with `$A$'.  We already have `$A$' as an assumption on line 1, but the only way to get it to appear at the end of our second subproof is to first conjoin it with `$B$' to get `$(A \eand B)$' on line 3, and then to use $\eand$E to get it back on its own on line 4.

In order to avoid having to use the trick of using $\eand$I and $\eand$E in this way to repeat earlier lines at later stages in a proof, we'll allow ourselves to use the following shortcut rule:
\factoidbox{\define{Reiteration Rule:} at any point in a proof, we may write down a sentence occurring on any line that (i) appears before that point in the proof, and (ii) isn't inside a closed subproof.
}
This lets us shorten the above proof to:\\

\begin{proof}
\open
\hypo{1}{A} \by{Assumption}{}
\open
\hypo{2}{B}  \by{Assumption}{}
\have{4}{A} \by{Reit}{1}
\close
\have{5}{B \eif A} \ci{2-4}
\close
\have{6}{A \eif (B \eif A)} \ci{1-5}
\end{proof}

\noindent Reiteration also gives us a quick way to prove that `$B \eif B$' is a theorem:\\

\begin{proof}
\open
\hypo{1}{B} \by{Assumption}{}
\have{2}{B} \by{Reit}{1}
\close
\have{3}{B \eif B} \ci{1-2}
\end{proof}

In fact, just as $\eand$I can be applied to a single line to go from `$A$' to `$A \eand A$', so $\eif$I can in principle be applied to a subproof that consists of just one line.  So an even shorter proof of  the theorem `$B \eif B$' can be given like this:\\

\begin{proof}
\open
\hypo{1}{B} \by{Assumption}{}
\close
\have{3}{B \eif B} \ci{1-1}
\end{proof}

\practiceproblems



\problempart
Prove the following theorems:

\begin{earg}
\item $\vdash (A \eand  B) \eif (B \eand  A)$
\item $\vdash (A\eif B)\eif ((B\eif C)\eif (A\eif C))$
\item $\vdash (A \eif (B \eif C)) \eif (B \eif (A \eif C))$
\item $\vdash A \eif (B \eif A)$ % REMOVE
\item $\vdash (A \eand  B) \eif (B \eand  A)$
\item $\vdash P \eif P$
\item $\vdash Q \eif (P \eif P)$
\end{earg}









\section{Biconditional Rules}
The rules for the biconditional will be like double-barrelled versions of the rules for the conditional.  In order to prove `$F \eiff G$', for instance, you must be able to prove `$G$' on the assumption `$F$' \emph{and} prove `$F$' on the assumption `$G$'. The biconditional introduction rule {\eiff}I therefore requires two subproofs. Schematically, the rule works like this:
\factoidbox{
\begin{proof}
	\open
		\hypo[i]{a1}{\meta{\varphi}}
		\have[\ ]{}{\vdots}
		\have[j]{b1}{\meta{\psi}}
	\close
	\open
		\hypo[k]{b2}{\meta{\psi}}
		\have[\ ]{}{\vdots}
		\have[l]{a2}{\meta{\varphi}}
	\close
	\have[\ ]{ab}{\meta{\varphi}\eiff\meta{\psi}}\bi{a1-b1,b2-a2}
\end{proof}}
There can be as many lines as you like between $i$ and $j$, and as many lines as you like between $k$ and $l$. Moreover, the subproofs can come in any order, and the second subproof does not need to come immediately after the first.

The biconditional elimination rule {\eiff}E is a bit like $\eif$E in both directions. If you have the left-hand subsentence of the biconditional, you can obtain the right-hand subsentence, and if you have the right-hand subsentence, you can obtain the left-hand subsentence:
\factoidbox{
\begin{proof}
	\have[m]{ab}{\meta{\varphi}\eiff\meta{\psi}}
	\have[n]{a}{\meta{\varphi}}
	\have[\ ]{b}{\meta{\psi}} \be{ab,a}
\end{proof}}
and:
\factoidbox{\begin{proof}
	\have[m]{ab}{\meta{\varphi}\eiff\meta{\psi}}
	\have[n]{a}{\meta{\psi}}
	\have[\ ]{b}{\meta{\varphi}} \be{ab,a}
\end{proof}}
As usual, lines $m$ and $n$ can occur in either order, but in the citation for $\eiff$E, we always cite the line number of the biconditional first.

Here's an example involving $\eiff$I:


\begin{proof}
\hypo{1}{A \eand B} \by{Premise}{}
\open
\hypo{2}{A}  \by{Assumption}{}
\have{3}{B} \ae{1}
\close
\open
\hypo{4}{B} \by{Assumption}{}
\have{5}{A} \ae{1}
\close
\have{6}{A \eiff B} \bi{2-3, 4-5}
\end{proof}

My reasoning for setting this up is that since it's my goal to prove `$A \eiff B$', I will have to create two subproofs, one for each direction of the biconditional. My first subproof (lines 2--3) begins with the assumption `$A$' and ends with `$B$', and my second subproof (lines 4--5) goes the other way, beginning with the assumption `$B$' and ending with `$A$'.

\practiceproblems

\problempart
Prove the following:

\begin{earg}
\item $A \eand B \vdash A \eiff B$
\item $A \eiff B \vdash B \eiff A$
\item $A \eiff B \vdash (A \eand C) \eiff (B \eand C)$
\item $(A \eand B) \eiff (A \eand C) \vdash A \eif (B \eiff C)$
\item $A\eiff B, B\eiff C \proves A\eiff C$
\item $K \eand L \proves K \eiff L$
\item $A \eiff B \vdash (A \eand C) \eiff (B \eand C)$
\item $(Z\eand K)\eiff(Y\eand M), D\eand(D\eif M) \proves Y\eif Z$
\item $\proves P \eiff P$
\end{earg}





\section{Negation Rules}



Our next connective is negation. In the context of natural deduction, negation is unusual because the rules governing it involve another notion, that of  \emph{contradiction}.

Consider this: an effective way to argue against someone is to show that the assumptions they are making collectively lead to a contradiction.  At that point, you have your opponent in a bind: since your opponent's assumptions lead to a contradiction, they can't all be true! So they have to give up at least one of those assumptions. This argumentative strategy exemplifies the \emph{reductio ad absurdum} reasoning that we encountered in \S\ref{s:PartTTableEnt}.  We are going to make use of this idea in our proof system by adding the absurdity symbol `$\ered$'.  You can think of it as officially declaring `contradiction!'\ or `reductio!'\ or `but that's absurd!'

We can declare a contradiction with \ered \ whenever we  have both a sentence and its negation appearing in our proof.  This gives us our negation elimination rule:
\factoidbox{
\begin{proof}
\have[m]{a}{\meta{\varphi}}
\have[n]{na}{\enot\meta{\varphi}}
\have[ ]{bot}{\ered}\ne{a, na}
\end{proof}}
It doesn't matter in what order the sentence and its negation appear in, and they don't need to appear on adjacent lines---as long as a sentence and its negation appear in your proof (and neither is trapped inside a closed subproof), you can declare a contradiction with \ered.  This rule is called $\lnot$E because a negation sign is eliminated in favor of $\bot$.  We could have equally well called this the \ered I rule, since it introduces \ered \ into the proof, but we'll continue to call it \enot E, following Gentzen, who first formulated these rules.


%I have said that `$\ered$' should be read as something like `contradiction!' But this does not tell us much about the symbol. There are, roughly, three ways to approach the symbol.
%	\begin{ebullet}
%		\item We might regard `$\ered$' as a new atomic sentence of TFL, but one which can only ever be assigned the value False.
%		\item We might regard `$\ered$' as an abbreviation for some canonical contradiction, such as `$A \eand \enot A$'. This will have the same effect as the above---obviously, `$A \eand \enot A$' only ever has the truth value False---but it means that, officially, we do not need to add a new symbol to TFL.
%		\item We might regard `$\ered$', not as a symbol of TFL, but as something more like a \emph{punctuation mark} that appears in our proofs. (It is on a par with the line numbers and the vertical lines, say.)
%	\end{ebullet}
%There is something philosophically attractive about the third option. But here we will officially go for the second option. `$\ered$' is to be read as abbreviating some canonical contradiction. This means that we can manipulate it in our proofs just like any other sentence.


Next, we turn to our rule of negation introduction. This rule will be a formal implementation of the \emph{reductio ad absurdum} proof strategy described earlier: if making an assumption leads you to a contradiction, then you know that assumption must be wrong, and you can infer its negation. The rule looks like this:
\factoidbox{\begin{proof}
		\open
		\hypo[i]{a}{\meta{\varphi}}
		\have[\ ]{}{\vdots}
		\have[j]{nb}{\ered}
		\close
		\have[\ ]{na}{\enot\meta{\varphi}}\ni{a-nb}
\end{proof}}
There can be as many lines between $i$ and $j$ as you like.  As with other rules that require subproofs, you need to cite the entire subproof when applying $\enot$I.  Notice that since the subproof has to end with the contradiction symbol $\ered$, we will usually have to use \enot E in the course of using $\enot$I.

Here's an example of how this works.  Suppose we want to show:
$$\enot A \proves \enot(A \eand B)$$
This should strike you as intuitively valid: if $A$ isn't true, then of course any conjunction containing $A$ can't be true either.  To prove this, what we'll do is assume that `$(A \eand B$)' \emph{is} true, derive a contradiction from this assumption together with our premise, and then conclude `$\enot (A \eand B)$' by $\enot$I.  That looks like this (the Reiteration step could be skipped too):
\begin{proof}
	\hypo{d}{\enot A} \by{Premise}{}
	\open
	\hypo{nd}{A \eand B} \by{Assumption}{}
	\have{e}{A}\ae{nd}
	\have{f}{\enot A}\by{Reit}{d}
	\have{ndr}{\ered}\ne{e,f}
	\close
	\have{con}{\enot(A \eand B)}\ni{nd-ndr}
\end{proof}

The strategy of reasoning by \emph{reductio ad absurdum} can take another form too, however. The $\enot$I rule says that in order to show that some sentence \meta{\varphi} is false (i.e. that $\enot \meta{\varphi}$ holds), we have to show that assuming \meta{\varphi} leads to a contradiction.  But instead of using \emph{reductio} to show that something is \emph{false}, we can also use it to show that something is \emph{true}: to prove that \meta{\varphi} is true, we suppose that it is false, i.e. assume $\enot \meta{\varphi}$, and reduce \emph{that} to a contradiction.



Interestingly, the rules we have assembled so far won't yet let us replicate this second style of reductio reasoning.  So we'll add it to our proof system as another primitive rule, which we'll call \emph{indirect proof}:
\factoidbox{\begin{proof}
		\open
		\hypo[i]{a}{\enot \meta{\varphi}}
		\have[\ ]{}{\vdots}
		\have[j]{nb}{\ered}
		\close
		\have[\ ]{na}{\meta{\varphi}}\ip{a-nb}
\end{proof}}
This is called indirect proof because it lets us prove \meta{\varphi} ``indirectly,'' by assuming its negation and deriving a contradiction from that assumption. It's quite similar to $\lnot$I, except that the location of the negation symbol gets reversed: instead of \emph{deriving} a negated sentence (as with $\enot$I), we instead \emph{assume} a negated sentence and then infer its un-negated counterpart.

It bears emphasis that IP is a very powerful rule, because any proof whatsoever can in principle be done using IP as the overall strategy!  Just assume the negation of \emph{whatever conclusion} you're trying to prove, derive a contradiction from that, and then infer your conclusion by IP.\footnote{In section \ref{s:PartTTableEnt}, on partial truth tables, we essentially did this kind of proof in English: we assumed that the premises are true and  that the conclusion false (i.e. that its negation  is true), and then showed that a contradiction resulted.  We can now do the same reasoning formally, using our rule of IP.}  But be careful: indirect proofs tend to be longer and more complicated than direct proofs.  So IP should only be used as a \emph{last resort}, when you're sure that there's no other way to complete the proof.

Here is an example of something that can \emph{only} be proven using IP:
$$P \eand \enot P \proves Q$$
The corresponding entailment $P \eand \enot P \entails Q$ holds: there is no valuation that makes the premise true and the conclusion false.  That's simply because no valuation makes the premise true!  So if our system of deduction is to be complete, we had better be able to provide a natural deduction proof of this as well.  We can do that with IP, as follows:\\

\begin{fitch}
\fj P\eand \enot P & Premise  \\
\fa \fh \enot Q & Assumption \\
\fa \fa P & $\eand$E 1 \\
\fa \fa \enot P & $\eand$E 1\\
\fa \fa \ered  & \enot E 3,4 \\
\fa Q &  IP 2-5 \\
\end{fitch}\\

\noindent This proof illustrates the \define{Explosion Principle}: from a contradiction, like $P \eand \enot P$, anything whatsoever follows!  We here proved $Q$, but exactly the same sequence of steps would have equally well let us prove $A$, or $D$, or $(S \eif L)$, or anything else.    The Explosion Principle can be stated in general terms as $\ered \proves \meta{\varphi}$.

Another thing that we can only prove using IP is the \define{Law of Excluded Middle}, which says that $\proves \meta{\varphi} \eor \enot \meta{\varphi}$, i.e. that any statement of the form $\meta{\varphi} \eor \enot \meta{\varphi}$ is a theorem.  Proponents of  \emph{intuitionistic logic} reject the Law of Excluded Middle, because they reject the assumption we've been making throughout this book, that for any statement, it or its negation must be true.  So they must reject our rule IP, since it lets us prove Excluded Middle.

There are also logicians who reject the Explosion Principle.  For example, proponents of \emph{relevance logic} hold that there must always be some ``relevant connection'' between the premises and conclusion of a valid argument  --- something the Explosion Principle violates.  And proponents of \emph{paraconsistent logic} hold the view that some contradictions are true, and that accepting a contradiction should therefore not allow you to infer anything whatsoever.  So they reject the Explosion Principle, and therefore have to use a different set of rules that doesn't prove this principle.  

Intuitionistic logic, relevance logic, and paraconsistent logic are all varieties of \define{non-classical logic}.   However, in \define{classical logic}, which is what we are here studying, both the Law of Excluded Middle and the Explosion Principle hold.  Since we can't prove these without IP, we have to add this rule into our natural deduction system in order to render it complete with respect to classical logic.  You can learn more about negation rules and completeness in Exercise \ref{ex-neg-prf-2} below.

Let's look at one more proof with IP.  \define{contraposition} is the following general equivalence law: $\meta{\varphi} \eif \meta{\psi} \lequiv \enot \meta{\psi} \eif \enot \meta{\varphi}$.  Let's prove an instance of the right-to-left direction: $\lnot B \eif \lnot A \vdash A \eif B$.  Since we're trying to prove a conditional, we'll use $\eif$I: we'll assume $A$ and try to prove $B$.  But how can we get $B$ from our premise $\lnot B \eif \lnot A$ together with our assumption $A$?  The only way to do it is by indirect proof, like this:\\

\begin{fitch}
\fj (\enot B\eif \enot A) & Premise\\
\fa \fh A & Assumption (for $\eif$I)\\
\fa \fa \fh \enot B & Assumption (for IP)\\
\fa \fa \fa \enot A & $\eif$E  1,3\\
\fa \fa \fa \bot  &  \enot E 2,4\\
\fa \fa B & IP  3-5\\
\fa (A\eif B) & $\eif$I  2-6\\
\end{fitch}\\

\noindent To prove $B$ via IP, I assumed $\enot B$, and showed that this, together with my assumption of $A$ and our premise 1, leads to a contradiction.  The other direction of contraposition holds too: $A \eif B \vdash \enot B \eif \enot A$.  Try proving this as well; here you'll be able to use $\lnot$I instead of IP.


\practiceproblems \label{ex-neg-prf-2}

\problempart Prove the following:


\begin{earg}
\item $A\eif B, A \eif \enot B \vdash \enot A$
\item $P \rightarrow \lnot Q \vdash Q \rightarrow \lnot P$
\item $A \ \eand  \ \enot B \vdash \enot (A \eif B)$ %REMOVE
\item $\enot (P\eand  Q) \vdash P\eif \enot Q$
\item $ \vdash \enot (A\eand  \enot A)$
\item $A \eif \enot B \vdash B \eif \enot A$
\item $A \eif B, B \eif \enot A \vdash \enot A$
\item $(A \eand B) \eif \enot A \vdash A \eif \enot B$
\item $\enot A \vdash A \eif B$
\item $(A \eand \enot B) \eif B \vdash A \eif B$
\item $(A\eiff \enot B) \vdash \enot (A\eiff B)$
\item $\enot (A\eand \enot B) \vdash A \eif B$
\item $P \eif Q \pequiv \enot Q \eif \enot P$ \hfill [Do both directions]
\item $\enot (P \eif Q) \pequiv (P \eand \enot Q)$ \hfill [Do both directions]
\item $P \pequiv \enot \enot P$ \hfill [Do both directions]
\item $\vdash \enot P \eif( P \eif Q)$
\item $\vdash (\enot A \eif A) \eif A$
\item $\vdash \enot (A \eand \enot A)$

\end{earg}

\problempart\label{ex-neg-prf}We've seen that if we only have \enot E and \enot I, our proof system is incomplete: we can't prove every valid argument to be valid. So we added IP as an additional rule.  In the following questions, we'll look more closely at what's needed to get a complete proof system.


\begin{earg}
\item It turns out that once we add IP into our proof system, we don't really need $\enot$I anymore, because anything we can prove using $\lnot$I can be proven using IP instead (though again, not the other way around!).  The core of $\lnot$I can be expressed as: $$A \rightarrow \bot \vdash \lnot A$$ Your challenge: prove $A \rightarrow \bot \vdash \lnot A$ using IP  instead of $\enot$I.  This shows that we never really have to use $\enot$I.\\



\item Instead of adding IP into our system, a more conservative approach would have been to add the rule of \emph{double negation elimination:}

\factoidbox{
\begin{fitch}
\ftag{i}{\fa \enot \enot \meta{\varphi}} & \\
\ftag{}{\fa \meta{\varphi}} & DNE  $i$ \\
\end{fitch}
}
That would have been another way to get a complete proof system, but without rendering $\enot$I idle.  The core of IP can be expressed as:
$$\enot A \eif \bot \proves  A$$
Your challenge: prove $\enot A \eif \bot \proves  A$ using DNE (together with our other rules) instead of IP.  This shows we could replace IP with DNE.\\

\item Another alternative to adding IP into our proof system would have been to add a pair of rules, one for the Explosion Principle:

\factoidbox{
\begin{fitch}
\ftag{i}{\fa \bot} & \\
\ftag{}{\fa \meta{\varphi}} & EX $i$ \\
\end{fitch}
}
and another that lets us write down any instance of the Law of Excluded Middle:
\factoidbox{
\begin{fitch}
\ftag{}{\fa \meta{\varphi \eor \enot \varphi}} & LEM \\
\end{fitch}
}
Your challenge: prove $\enot A \eif \bot \proves  A$ using EX and LEM (together with our other rules)  instead of IP.  This shows we could replace IP with this pair of rules.  Note: to do this problem, you'll have to know how to work with disjunction rules, so read the next section on the $\eor$E rule before attempting  it.

\end{earg}


\section{Disjunction Rules}
Our last connective to deal with is disjunction.  The $\eor$I rule is pretty straightforward.  Suppose Alice is a logician. Then certainly Alice is either a logician or a chemist. After all, to say that Alice is either a logician or a chemist is to say something weaker than to say that Alice is a logician. In fact, we can weaken the claim however we like. Suppose Alice a logician. It follows that Alice is \emph{either} a logician \emph{or} a kumquat.   Equally, it follows that \emph{either} Alice is a logician \emph{or} the earth is flat. Many of these are strange inferences to draw. But there is nothing \emph{logically} wrong with them: in each case the conclusion has to be true if the premise that Alice is a logician is true.


Our disjunction introduction rules implement this idea of arbitrarily weakening a claim:
\factoidbox{\begin{proof}
	\have[m]{a}{\meta{\varphi}}
	\have[\ ]{ab}{\meta{\varphi}\eor\meta{\psi}}\oi{a}
\end{proof}}
and
\factoidbox{\begin{proof}
	\have[m]{a}{\meta{\varphi}}
	\have[\ ]{ba}{\meta{\psi}\eor\meta{\varphi}}\oi{a}
\end{proof}}
Notice that \meta{\psi} can be \emph{any} sentence. So the following is a perfectly kosher use of $\eor$I:
\begin{proof}
	\hypo{m}{M}
	\have{mmm}{M \eor ([(A\eiff B) \eif (C \eand D)] \eiff [E \eand F])}\oi{m}
\end{proof}


The disjunction elimination rule is slightly trickier. Suppose you know that Alice is either a logician or a chemist. What can you conclude? Not that Alice is a logician; she might be a chemist instead. And equally, not that Alice is a chemist; she might be a logician instead. Disjunctive premises, just by themselves, are hard to work with!

But suppose that we could somehow show both of the following: first, that Alice's being a logician implies that she has a PhD; second, that Alice's being a chemist also implies that she has a PhD. Then if we know that Alice is either a logician or a chemist, we know that, whichever she happens to be, she has a PhD. Our disjunction elimination rule $\eor$E formalizes this insight:
\factoidbox{
	\begin{proof}
		\have[m]{ab}{\meta{\varphi}\eor\meta{\psi}}
		\open
			\hypo[i]{a}{\meta{\varphi}} {}
			\have[\ ]{}{\vdots}
			\have[j]{c1}{\meta{\chi}}
		\close
		\open
			\hypo[k]{b}{\meta{\psi}}{}
			\have[\ ]{}{\vdots}
			\have[l]{c2}{\meta{\chi}}
		\close
		\have[ ]{c}{\meta{\chi}}\oe{ab, a-c1,b-c2}
	\end{proof}}
This is a bit more complicated than our previous rules, but the idea is fairly simple. Suppose we have some disjunction $\meta{\varphi} \eor \meta{\psi}$ and our goal is to prove some claim $\chi$.  If we can give two subproofs, one showing that our goal $\meta{\chi}$ follows from the assumption that $\meta{\varphi}$ holds, and another showing that the \emph{same} conclusion $\meta{\chi}$ \emph{also} follows from the assumption that $\meta{\psi}$ holds, then we can infer $\meta{\chi}$ itself by $\eor$E. This rule formally implements a proof strategy called \emph{argument by cases}: the disjunction $\meta{\varphi} \eor \meta{\psi}$ tells us that one of two cases obtains, either \meta{\varphi} holds or \meta{\psi} does; if it can now be shown that \meta{\chi} must hold in \emph{either case}, then we can conclude that \meta{\chi} holds on the basis of the original disjunction.

Notice that the citation for $\eor$E is quite complex.  We have to cite \emph{three} things: the line number of the original disjunction, and the two subproofs.  As usual, there can be as many lines as you like between $i$ and $j$, and as many lines as you like between $k$ and $l$. Moreover, the subproofs and the disjunction can come in any order, and do not have to be adjacent.

Some examples will help illustrate the rule. Consider this problem:
$$(P \eand Q) \eor (P \eand R) \vdash P$$
The premise tells us that either `$(P \eand Q)$' holds or `$(P \eand R)$' holds.  But in either case, `$P$' must hold, so `$P$' follows from our disjunctive premise.  Here's how this looks as a proof:
	\begin{proof}
		\hypo{prem}{(P \eand Q) \eor (P \eand R) }\by{Premise}{}
			\open
				\hypo{pq}{P \eand Q} \by{Assumption}{}
				\have{p1}{P}\ae{pq}
			\close
			\open
				\hypo{pr}{P \eand R} \by{Assumption}{}
				\have{p2}{P}\ae{pr}
			\close
		\have{con}{P}\oe{prem, pq-p1, pr-p2}
	\end{proof}

$\eor$E is similar to rules like $\eif$I, $\eiff$I, and $\enot$I, in that it requires temporary assumptions and subproofs.  But it differs in that, unlike the others, it's an \emph{elimination} rule!  This has an important consequence for the kind of strategy to use in relation to $\eor$E.  In the case of rules like $\eif$I and $\enot$I, we always reasoned ``from the bottom up,'' that is, we reasoned \emph{backward} from our goal. E.g. if our goal is to prove a conditional $\meta{\varphi} \eif \meta{\psi}$,  we know to use $\eif$I as the overall strategy, which means assuming $\meta{\varphi}$ and then proving $\meta{\psi}$.

The $\eor$E rule is the exception to working backwards like this. Here, we have to reason ``from the top down.'' That is, if you have  a disjunction $\meta{\varphi} \eor \meta{\psi}$ as a premise (or as an assumption, or as something you can easily derive from your premises and assumptions), it's usually a good idea to prove your goal, \emph{whatever it may be}, using $\eor$E as your overall strategy.  This means you have to open up two subproofs, and prove your goal $\meta{\chi}$ first from the left-disjunct $\meta{\varphi}$ and then from the right-disjunct $\meta{\psi}$.


Here's a more complex example, demonstrating one of the Distributive Laws:
	$$P \eor (Q \eand R) \vdash (P \eor Q) \eand (P \eor R)$$
\begin{center}
\begin{fitch}
\fj P\lor (Q\land R) & Premise\\
\fa \fh P & Assumption\\
\fa \fa (P\lor Q) & $\lor$I  2\\
\fa \fa (P\lor R) & $\lor$I  2\\
\fa \fa (P\lor Q)\land (P\lor R) & $\land$I  3,4\\
\fa \fh (Q\land R) & Assumption\\
\fa \fa Q & $\land$E  6\\
\fa \fa R & $\land$E  6\\
\fa \fa (P\lor Q) & $\lor$I  7\\
\fa \fa (P\lor R) & $\lor$I  8\\
\fa \fa (P\lor Q)\land (P\lor R) & $\land$I  9,10\\
\fa (P\lor Q)\land (P\lor R) & $\lor$E  1,2-5,6-11\\
\end{fitch}\end{center}

To identify my overall strategy in this case, I did \emph{not} look at my goal $(P\lor Q)\land (P\lor R)$ and think about what introduction rule I might use to prove it (e.g. $\eand$I).  Rather, I noticed that my premise $P \eor (Q \eand R)$ is a disjunction.  And whenever I have a disjunction as a premise like this (or as an assumption, or as something easily derivable from my premises and assumptions), I will prove my current goal, whatever it might be, using $\eor$E as the overall strategy. So in this case, that means first assuming the left disjunction $P$ of my premise, and proving my conclusion from that (lines 2--5), and then assuming the right disjunct $(Q \eand R)$ of my premise, and proving my conclusion from that (lines 6-11).



\practiceproblems

\problempart
Prove the following:

\begin{earg}
\item $A \eif B \proves A \eif (C \eor B)$
\item $(B \eor A) \eif C \vdash A \eif C$
\item $(A \eand B) \eor (A \eand C) \vdash A$
\item $A \eor B \proves B \eor A$
\item  $A \eor B \vdash (A\eif B) \eif B$ 
\item $A \eor (B \eand C) \pequiv (A \eor B) \eand (A \eor C)$
\item $ A \eand (B \eor C) \pequiv (A \eand B) \eor (A \eand C)$
\item $A \eor (B \eor C) \vdash (A \eor B) \eor C$
\item $S\eiff T \proves S\eiff (T\eor S)$
\item $(A \eor B) \eor C, \ C \eiff B \proves C \eor A$
\item $(C\eand D)\eor E \proves E\eor D$
\item $A \eif  C \vdash (A \eor (B \eand C)) \eif C$ 
\item $D \eif A, C \eif (B \eand A) \proves (D \eor C) \eif (B \eor A)$
\item $A \eor B \proves (A \eif B) \eif B$
\item $(Z\eand K) \eor (K\eand M), K \eif D \proves D$
\end{earg}



\section{Proof strategies}\label{s:ProofStrategies}
There is no simple recipe for proofs, and there is no substitute for practice. But here are some strategies to keep in mind.

\paragraph{Work backwards from your goal.}
Your ultimate goal is arrive at the conclusion. Look at the conclusion and ask what the introduction rule is for its main logical operator. This gives you an overall strategy, and tells you what assumptions  (if any)  to make, and what your new goal is. Now ask what the main operator of that new goal is, thereby identifying a strategy to prove it, and so on.


For example: if your conclusion is a conditional $\meta{\varphi}\eif\meta{\psi}$, you should plan to use the {\eif}I as your strategy. This requires opening a subproof in which you assume \meta{\varphi} and then set \meta{\psi} as your new goal.  Or if your goal is to prove $\meta{\varphi} \eiff \meta{\psi}$, use $\eiff$I as your overall strategy (which involves giving two subproof, one in each direction of the arrow). Or if your conclusion is a negated sentence $\enot \meta{\varphi}$, plan to use $\enot$I as your strategy (which means assuming \meta{\varphi} and proving a contradiction).


\paragraph{The Exception: $\eor$E}
The one important exception to the strategy of working backwards involves the $\eor$E rule: if you see a disjunction $\meta{\varphi} \eor \meta{\psi}$ among your premises or assumptions (or as something that you can easily derive from them), it's almost always a good idea to prove your current goal (whatever it may be) by setting up an $\eor$E proof.  That means opening up two subproofs, one that begins with an assumption of \meta{\varphi} and another that begins with \meta{\psi}.  Inside each of them, you now have to prove whatever your current goal is.



\paragraph{Try an indirect proof.}
If you can't find any way to prove your goal $\meta{\varphi}$ directly, try an indirect proof: assume $\enot \meta{\varphi}$ and try to derive a contradiction.  If you succeed, you can infer your goal \meta{\varphi} by IP.  This strategy should only be used as a last resort, however!  Indirect proofs are often longer and more complicated than direct proofs.

\paragraph{Persist.}
Try different things. If one approach fails, try something else.  I'll never ask you to prove something that cannot be proven.

\paragraph{Don't make random assumptions.} Finally, never make an assumption unless you have a strategy in mind for discharging it (i.e. for ultimately closing the subproof that the assumptions opens).  That means you should only make an assumption if your strategy is to use one of the following discharge rules: $\eif$I, $\eiff$I, $\enot$I, IP, or $\eor$E.\\


Let's look at one more example.  Take the following English argument:
\begin{quote}
If Guatemala is in Canada, then it is in North America. So if Guatemala is not in North America, it also isn't in Canada.
\end{quote}
This is intuitively valid, so we should be able to give a proof of it.  We can symbolize the argument as: $C \eif A \therefore \enot A \eif \enot C$.	Our goal here is to prove a conditional, `$\enot A \eif \enot C$'.  So we use $\eif$I as our strategy, meaning we assume `$\enot A$' and set ourselves the new goal of proving `$\enot C$' from that assumption.  And now, since `$\enot C$' has a negation as its main operator, we use $\enot$I as our strategy, meaning we assume `$C$' and prove a contradiction from that.  Notice how I reasoned ``backwards'' from the conclusion in order to discover this strategy.  Written out, the proof looks like this:


\begin{proof}
\hypo{1}{C \eif A}  \by{Premise}{}
\open
	\hypo{2}{\enot A} \by{Assumption (for $\eif$I)}{}
	\open
		\hypo{3}{C} \by{Assumption (for $\enot$I)}{}
		\have{4}{A} \ce{1,3}
		\have{5}{\ered} \ne{2,4}
	\close
	\have{6}{\enot C} \ni{3-5}
\close
\have{7}{\enot A \eif \enot C} \ci{2-6}
\end{proof}



\practiceproblems
\problempart
The following three proofs are missing their rule citations. Write them in. Additionally, write down the sequent (i.e. single-turnstile $\proves$ statement) that each proof demonstrates.
\begin{multicols}{2}
\begin{proof}
\hypo{ps}{P \eand S}
\hypo{nsor}{S \eif R}
\have{p}{P}%\ae{ps}
\have{s}{S}%\ae{ps}
\have{r}{R}%\ce{nsor, s}
\have{re}{R \eor E}%\oi{r}
\end{proof}

\begin{proof}
\hypo{ad}{A \eif D}
\open
	\hypo{ab}{A \eand B}
	\have{a}{A}%\ae{ab}
	\have{d}{D}%\ce{ad, a}
	\have{de}{D \eor E}%\oi{d}
\close
\have{conc}{(A \eand B) \eif (D \eor E)}%\ci{ab-de}
\end{proof}

\begin{proof}
\hypo{nlcjol}{\enot L \eif (J \eor L)}
\hypo{nl}{\enot L}
\have{jol}{J \eor L}%\ce{nlcjol, nl}
\open
	\hypo{j}{J}
	\have{jj}{J \eand J}%\ai{j}
	\have{j2}{J}%\ae{jj}
\close
\open
	\hypo{l}{L}
	\have{red}{\ered}%\ne{l, nl}
	\have{j3}{J}%\re{red}
\close
\have{conc}{J}%\oe{jol, j-j2, l-j3}
\end{proof}
\end{multicols}

\problempart
Prove each of the following:
\begin{earg}
\item $J\eif\enot J \proves \enot J$
\item $Q\eif(Q\eand\enot Q) \proves \enot Q$
\item $P \eor Q, \enot P \proves Q$
\item $\enot R \eor (P \eif Q) \vdash (R \eand P) \eif Q$
\item $\enot F\eif G, F\eif H \proves G\eor H$
\item $D \proves \enot \enot D$
\item $P \eand (Q\eor R), P\eif \enot R \proves Q\eor E$
\item $\enot C \eor (A \eif B) \proves (C \eand A) \eif B$
\item $C\eif(E\eand G), \enot C \eif G \proves G$
\item $M \eand (\enot N \eif \enot M) \proves (N \eand M) \eor \enot M$
\item $(W \eor X) \eor (Y \eor Z), X\eif Y, \enot Z \proves W\eor Y$


\end{earg}




\problempart
Show that the following are provably equivalent:
\begin{earg}
\item $\enot (P \eand Q) \pequiv \enot P \eor \enot Q$
\item $\enot (P \eor Q) \pequiv \enot P \eand \enot Q$
\item $P \eor Q \pequiv \enot (\enot P \eand \enot Q)$
\item $P \eif Q \pequiv \enot Q \eif \enot P$
\item $P \eif Q \pequiv \enot P \eor Q$
\item $\enot (P \eif Q) \pequiv P \eand \enot Q$
\item $P \eiff \enot Q \pequiv \enot(P \eiff Q)$
\item $P \eor Q \pequiv P \eor (\enot P \eand Q)$
\end{earg}

\problempart
Prove the following theorems:

\begin{earg}
\item $\proves (P \eif Q) \eor (Q \eif P)$
\item $\proves A \eor \enot A$ %\hfill This is the \emph{Law of Excluded Middle}
\item $\proves ((P \eif Q) \eif P) \eif P$ %\hfill This is \emph{Peirce's Law}
\item $\proves \enot A \eif (A \eif B)$
\item $\proves J \eiff [J\eor (L\eand\enot L)]$
\end{earg}



\section{Derived Rules}\label{s:TFLDerivedRules}

We have provided introduction and elimination rules for each of our five connectives.  Together with IP, this gives us a complete proof system: every valid argument can be proven using just these few basic rules!  In this section, we're going to introduce some additional rules to shorten our proofs and make our proof system easier to work with.  It's important to note at the outset that these additional rules are not necessary.  They represent a \emph{conservative} extension of our proof system: anything proven using these new rules can also be proven using just our basic set of rules.

To illustrate the motivation for additional rules, consider the following argument:
	\begin{quote}
		Alice is either a logician or a chemist. She is not a chemist.  So she is a logician.
	\end{quote}
This involves a very natural form of inference called \emph{Disjunctive Syllogism}.  We could symbolize the argument as $L\eor C, \enot C \therefore L$, and we then give a natural deduction proof using \eor E to show that it is valid.

But now consider this: by giving a proof of `$L$' from `$L \eor C$' and `$\enot C$', we have implicitly shown that given \emph{any} sentences of the form $\meta{\varphi} \eor \meta{\psi}$ and $\enot \meta{\psi}$, it is possible to prove \meta{\varphi}.  If we substitute the metavariables $\meta{\varphi}$ and $\meta{\psi}$ for the sentences  `$L$' and `$C$' in our proof, we get a \define{proof template} for the disjunctive syllogism form of inference:\\



\begin{fitch}
\ftag{m}{\fa (\meta{\varphi}\eor \meta{\psi})} & \\
\ftag{n}{\fa \enot \meta{\varphi} } & \\
\ftag{k_0}{\fa \fh \meta{\varphi}} & \\
\ftag{k_1}{\fa \fa \fh \enot \meta{\psi}} & \\
\ftag{k_2}{\fa \fa \fa \ered  }& \enot E $n, k_0$ \\
\ftag{k_3}{\fa \fa \meta{\psi}} &  IP $k_0$--$k_3$\\
\ftag{k_4}{\fa \fh \meta{\psi} } & \\
\ftag{k_5}{\fa \fa \meta{\psi} \eand \meta{\psi}} &  $\eand$I $k_4, k_4$ \\
\ftag{k_6}{\fa \fa \meta{\psi}} &  $\eand$E $k_5$ \\
\ftag{k_7}{\fa \meta{\psi} }&  $\eor$E $m, k-k_3, k_4-k_6$\\
\end{fitch}\\


\noindent Now, if at any time, in the context of any proof whatsoever, we need to prove some sentence \meta{\varphi} from two sentences of the form $\meta{\varphi} \eor \meta{\psi}$ and $\enot \meta{\psi}$, we can simply ``slot in'' an instance of the above proof template.  In other words, once we've proven one instance of disjunctive syllogism using our basic rules, we can use that as a template to prove disjunctive syllogism again in the context of any other proof.

Given this, we might as well just introduce a \define{derived rule} into our proof system that lets us skip the actual proof and make the disjunctive syllogism inference \emph{directly}:
\factoidbox{\begin{proof}
	\have[m]{ab}{\meta{\varphi} \eor \meta{\psi}}
	\have[n]{nb}{\enot \meta{\varphi}}
	\have[\ ]{con}{\meta{\psi}}\by{DS}{ab, nb}
\end{proof}}
DS is a \define{derived rule} in the sense that it can be shown to hold using only the \emph{primitive} rules of our system.    You can think of derived rules like promissory notes: ``I am here justifying my inference by writing `DS', but I promise that, if you asked for it, I could slot in a series of steps using only the primitive rules of our natural deduction system.''  Derived rules shorten our proofs, but add no power into our proof system: any proof that appeals to a derived rule could be expanded into one that only appeals to primitive rules.

In fact, we implicitly already added a derived rule to our system in \S\ref{s:TheoremsReiterating} when we introduced Reiteration.  Reiteration is just a shortcut to let us skip the $\eand$I-plus-$\eand$E trick that I used in steps $k_5$ and $k_6$ in the above proof template.\footnote{Indeed, in this particular case we could have avoided using the $\eand$I+$\eand$E trick, and shortened our proof template, by treating line $k+4$ as a whole subproof that begins with \meta{\psi} and ends with \meta{\psi} (see the end of \S\ref{s:TheoremsReiterating}).}  There are many further useful derived rules we can add to our proof system.  For example, consider the following argument:
	\begin{quote}
		If Alice is a chemist, then she has a PhD. Alice doesn't have a PhD. So she isn't a chemist.
	\end{quote}
This inference pattern is called \emph{modus tollens}, and we can introduce a derived rule for it:
\factoidbox{\begin{proof}
	\have[m]{ab}{\meta{\varphi}\eif\meta{\psi}}
	\have[n]{a}{\enot\meta{\psi}}
	\have[\ ]{b}{\enot\meta{\varphi}}\mt{ab,a}
\end{proof}}
Again, this adds no power to our system because it is simply a shortcut for a series of steps involving only primitive rules, as illustrated by the following proof template:
\begin{proof}
	\have[m]{ab}{\meta{\varphi}\eif\meta{\psi}}
	\have[n]{nb}{\enot\meta{\psi}}
		\open
		\hypo[k_0]{a}{\meta{\varphi}}
		\have[k_1]{c}{\meta{\psi}}\ce{ab, a}
		\have[k_2]{d}{\ered}\ne{c, nb}
		\close
	\have[k_3]{e}{\enot\meta{\varphi}}\ni{a-d}
\end{proof}

In  \S\ref{s:ProofStrategies} we gave a seven step proof showing that $C \eif A \therefore \enot A \eif \enot C$ is valid.  Using our derived rule MT, we can now shorten this to just four steps:

\begin{proof}
\hypo{1}{C \eif A}  \by{Premise}{}
\open
	\hypo{2}{\enot A} \by{Assumption}{}
	\have{6}{\enot C} \by{MT}{1,2}
\close
\have{7}{\enot A \eif \enot C} \ci{2-6}
\end{proof}
Here is a complete list of the derived rules that you'll be able to use:

\begin{center}
\begin{tabular}{l  r}
\textbf{Sequent}                    &       \textbf{Derived Rule} \\ \hline
$\meta{\varphi} \rightarrow \meta{\psi},  \enot\meta{\psi} \proves \enot \meta{\varphi}$   &                       MT  \\
$\meta{\varphi} \eor \meta{\psi},  \enot\meta{\psi} \proves \meta{\varphi}$ & DS\\
$\meta{\varphi} \eor \meta{\psi},  \enot\meta{\varphi}\proves \meta{\psi}$    &      DS  \\
$\meta{\varphi} \proves\meta{\psi} \rightarrow \meta{\varphi}$  &              PMI  \\
$\enot\meta{\varphi}\proves\meta{\varphi}\rightarrow \meta{\psi}$  & PMI\\
$\meta{\varphi} \rightarrow\meta{\psi} \pequiv \enot\meta{\varphi}\eor \meta{\psi}$  &                   Imp   \\
$\enot (\meta{\varphi} \rightarrow \meta{\psi}) \pequiv \meta{\varphi} \eand \enot \meta{\psi}$  &               NegImp  \\
$\enot (\meta{\varphi} \eand \meta{\psi})  \pequiv \enot\meta{\varphi}\eor \enot \meta{\psi}$  &                DeM  \\
$\enot (\meta{\varphi} \eor \meta{\psi}) \pequiv \enot\meta{\varphi}\ \eand \ \enot \meta{\psi}$   &               DeM  \\
$\meta{\varphi} \pequiv \enot \enot \meta{\varphi}$     &                              DN  \\
$(\meta{\varphi}  \ \#\  \meta{\psi}) \pequiv (\enot \enot \meta{\varphi}  \ \#\   \enot \enot \meta{\psi}) \pequiv (\enot \enot \meta{\varphi}  \ \#\  \meta{\psi}) \pequiv (\meta{\varphi}  \ \#\  \enot \enot  \meta{\psi})$ & SDN\\
$\enot (\meta{\varphi}  \ \#\  \meta{\psi}) \pequiv \enot (\enot \enot \meta{\varphi}  \ \#\   \enot \enot \meta{\psi}) \pequiv \enot (\enot \enot \meta{\varphi}  \ \#\  \meta{\psi}) \pequiv \enot (\meta{\varphi}  \ \#\  \enot \enot  \meta{\psi})$ & SDN\\
$\meta{\varphi} @ \meta{\psi}  \proves  \meta{\psi} @ \meta{\varphi}$ &                          Com \\
$\bot \proves \meta{\varphi}$ & EX \\
 $\proves \meta{\varphi}\eor \enot \meta{\varphi}$ &                                                    LEM \\
\end{tabular}
\end{center}

\noindent (where  \ \#\  in SDN can be any binary connective, and $@$ Com can be any of the three commutative connectives $\eor, \eand, \eiff$).  The way understand this list of derived rules is as follows:

\begin{itemize}
\item For any sequent $\meta{\varphi}_1 \ldots \meta{\varphi}_n \proves \meta{\psi}$ matching one from the above list, if $\meta{\varphi}_1 \ldots \meta{\varphi}_n$ occur on some earlier lines $j_1 \ldots j_n$ in your proof (none of them inside a closed subproof), then you may directly infer \meta{\psi} and justify it by citing the name of the relevant derived rule followed by the numbers of lines $j_1 \ldots j_n$.
\end{itemize}
For example, if you have the sentence `$P$' on line $m$ in your proof, then you are allowed to directly infer `$Q \eif P$' with the justification `PMI $m$' (for ``Paradox of Material Implication'').   Or if you have a sentence `$\enot K \eor \enot L$' on  line $m$ in you proof, you may directly infer `$\enot(K \eand L)$' with the justification `DeM $m$' (for ``DeMorgan's Law''), or `$K \eif \enot L$' with the justification `Imp $m$', or `$\enot L \eor \enot K$' with the justification `Com $m$'.  Notice that these latter three are examples of derived rules that hold in \emph{both} directions, so you could also start with `$\enot(K \eand L)$', or `$K \eif \enot L$', or $\enot L \eor \enot K$' and derive `$\enot K \eor \enot L$'.

The second-to-last derived rule, EX, is the Explosion Principle: it lets you infer any sentence whatsoever from a contradiction.  The last one, LEM, lets you write down any instance of the Law of Excluded Middle $\meta{\varphi} \eor \enot \meta{\varphi}$ at any point in your proof with the justification `LEM'.   Here no line number needs to be cited because you're introducing a theorem.

For another example of these rules in action, consider the following theorem:
$$\proves (P \eif Q) \eor (Q \eif P)$$
This was one of the exercises in \S\ref{s:ProofStrategies}.  Proving this using only basic rules is quite difficult, as you will have noticed if you tried that exercise.  With derived rules, we can give a much quicker and more intuitive proof of this theorem, by starting out with $P \eor \enot P$ as an instance of the Law of Excluded Middle, and then pursuing an $\eor$E strategy from there:\\


\begin{fitch}
\fa P\eor \enot P & LEM\\
\fa \fh P & Assumption (for $\eor$E) \\
\fa \fa Q\eif P & PMI  2\\
\fa \fa (P\eif Q)\eor (Q\eif P) & $\eor$I  3\\
\fa \fh \enot P & Assumption (for $\eor$E) \\
\fa \fa P\eif Q & PMI  5\\
\fa \fa (P\eif Q)\eor (Q\eif P) & $\eor$I  6\\
\fa (P\eif Q)\eor (Q\eif P) & $\eor$E  1,2-4,5-7\\
\end{fitch}\\

As an exercise, you might try to re-write this proof by ``slotting in'' a subproof involving only primitive rules wherever the above proof appeals to a derived rule.  This involves showing how LEM, and the two versions of PMI, can be proven using only primitive rules of our system.





\practiceproblems
\problempart
\label{pr.justifyTFLproof}
The following proofs are missing their citations (rule and line numbers). Add them wherever they are required:
\begin{multicols}{2}
\begin{proof}
\hypo{1}{W \eif \enot B}
\hypo{2}{A \eand W}
\hypo{2b}{B \eor (J \eand K)}
\have{3}{W}{}
\have{4}{\enot B} {}
\have{5}{J \eand K} {}
\have{6}{K}{}
\end{proof}
\vfill
\begin{proof}
\hypo{1}{L \eiff \enot O}
\hypo{2}{L \eor \enot O}
\open
	\hypo{a1}{\enot L}
	\have{a2}{\enot O}{}
	\have{a3}{L}{}
	\have{a4}{\ered}{}
\close
\have{3}{L}{}
\end{proof}
\columnbreak

\begin{fitch}
\fa Z\eif (C\eand \enot N) & \\
\fj \enot Z\eif (N\eand \enot C) & \\
\fa \fh \enot (N\eor C) & \\
\fa \fa \enot N\eand \enot C & \\
\fa \fa \enot N & \\
\fa \fa \enot N\eor \enot \enot C & \\
\fa \fa \enot (N\eand \enot C) & \\
\fa \fa \enot \enot Z &\\
\fa \fa Z &  \\
\fa \fa \enot C & \\
\fa \fa \enot C\eor \enot \enot N &\\
\fa \fa \enot (C\eand \enot N) & \\
\fa \fa \enot Z & \\
\fa \fa \ered  & \\
\fa N\eor C & \\
\end{fitch}


\end{multicols}

\problempart
Prove the following using derived rules:
\begin{earg}
\item $(A \eor B) \eif C , \enot C \proves \enot A$
\item $E\eor F$, $F\eor G$, $\enot F \proves E \eand G$
\item $\enot (A \eif (B \eor \enot C)) \proves (B \eor C) \eif A$
\item $(Q \rightarrow P) \rightarrow R, \lnot Q \lor \lnot S \vdash S \rightarrow \lnot (\lnot R \lor \lnot S)$
\item $M\eor(N\eif M)  \proves  \enot M \eif \enot N$
\item $A \eif (B \eor C) \proves (A \eif B) \eor (A \eif C)$
\item $(M \eor N) \eand (O \eor P)$, $N \eif P$, $\enot P  \proves  M\eand O$
\item $(B \eand C) \eor \enot (A \eif \enot D), \enot C \proves A \eand D$
\item $(X\eand Y)\eor(X\eand Z)$, $\enot(X\eand D)$, $D\eor M  \proves  M$
\item $\vdash (A \eif B) \eor (B \eif A)$

\end{earg}
If you want more practice, you can of course also re-do any of the earlier proofs in this chapter using derived rules. \\

\problempart
Provide proof templates (like those I provided for DS and MT) that justify the addition of the De Morgan rules, the Imp and NegImp rules, and LEM as derived rules. If you don't want to bother with metavariables, you can just prove instances of the corresponding sequents, but  in any case, be sure to only use primitive rules in your proofs.




